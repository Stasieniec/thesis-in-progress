#!/usr/bin/env python3
"""
Test the FIXED sMRI system using improved architecture with real data.
This should match our 96% synthetic performance on real ABIDE data.
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent / 'src'))

import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, WeightedRandomSampler
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report
import warnings
warnings.filterwarnings('ignore')

# Set seeds
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)

from models.smri_transformer import SMRITransformer  # This is now the improved version!
from data import SMRIDataset

def load_real_abide_data():
    """Load real ABIDE sMRI data if available."""
    print("üî¨ Loading real ABIDE sMRI data...")
    
    # Common paths where the data might be
    possible_paths = [
        "data/processed_smri_features.npy",
        "/content/drive/MyDrive/processed_smri_features.npy",
        "../data/processed_smri_features.npy",
        "test_smri_output/processed_features.npy"
    ]
    
    for features_path in possible_paths:
        if Path(features_path).exists():
            labels_path = features_path.replace('features', 'labels')
            if Path(labels_path).exists():
                features = np.load(features_path)
                labels = np.load(labels_path)
                print(f"   ‚úÖ Found real data at {features_path}")
                print(f"   üìä Shape: {features.shape}, Labels: {len(labels)}")
                return features, labels, True
    
    print("   ‚ö†Ô∏è  Real data not found, creating high-fidelity synthetic ABIDE data...")
    features, labels = create_abide_like_data()
    return features, labels, False

def create_abide_like_data():
    """Create high-fidelity ABIDE-like data based on real dataset characteristics."""
    print("   üîß Creating ABIDE-like synthetic data...")
    
    # Real ABIDE characteristics
    n_samples = 870
    n_features = 694
    n_asd = 403
    n_control = 467
    
    np.random.seed(SEED)
    
    # Create realistic FreeSurfer-like features with proper scaling
    features = np.random.randn(n_samples, n_features)
    
    # Volume features (first 200): Large values, log-normal distribution
    features[:, :200] = np.exp(features[:, :200] * 0.3 + np.log(5000))
    
    # Area features (200-400): Medium values
    features[:, 200:400] = np.exp(features[:, 200:400] * 0.2 + np.log(2000))
    
    # Thickness features (400-600): Small values, normal distribution
    features[:, 400:600] = features[:, 400:600] * 0.3 + 2.5
    
    # Other features (600+): Mixed
    features[:, 600:] = features[:, 600:] * 2
    
    # Create realistic group differences with effect sizes found in autism research
    asd_effects = np.random.randn(n_features) * 0.1  # Small to medium effect sizes
    
    # Some features have larger effects (consistent with research)
    high_effect_indices = np.random.choice(n_features, size=50, replace=False)
    asd_effects[high_effect_indices] *= 2
    
    # Apply group differences
    labels = np.concatenate([np.ones(n_asd), np.zeros(n_control)])
    for i, label in enumerate(labels):
        if label == 1:  # ASD
            features[i] += asd_effects
    
    # Shuffle data
    indices = np.random.permutation(len(labels))
    features = features[indices]
    labels = labels[indices]
    
    print(f"   ‚úÖ Created {n_samples} subjects, {n_features} features")
    print(f"   üéØ ASD: {np.sum(labels==1)}, Control: {np.sum(labels==0)}")
    
    return features, labels

def enhanced_preprocessing_real_data(X_train, X_val, X_test, y_train, n_features=300):
    """Enhanced preprocessing optimized for real ABIDE data."""
    print(f"üîß Enhanced preprocessing (real data optimized)...")
    
    # Handle outliers and invalid values (crucial for real FreeSurfer data)
    def clean_features(X):
        X = np.nan_to_num(X, nan=0.0, posinf=1e6, neginf=-1e6)
        # Remove extreme outliers (beyond 5 standard deviations)
        Q1 = np.percentile(X, 25, axis=0)
        Q3 = np.percentile(X, 75, axis=0)
        IQR = Q3 - Q1
        lower_bound = Q1 - 3 * IQR
        upper_bound = Q3 + 3 * IQR
        X = np.clip(X, lower_bound, upper_bound)
        return X
    
    X_train = clean_features(X_train)
    X_val = clean_features(X_val)
    X_test = clean_features(X_test)
    
    # RobustScaler (proven best for FreeSurfer data)
    print("   Applying RobustScaler (optimal for FreeSurfer outliers)...")
    scaler = RobustScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)
    
    # Enhanced feature selection (combined F-score + MI from data creation script)
    print("   Selecting features using combined F-score + MI (data creation optimized)...")
    
    # Calculate both metrics
    f_scores, _ = f_classif(X_train_scaled, y_train)
    mi_scores = mutual_info_classif(X_train_scaled, y_train, random_state=SEED)
    
    # Normalize scores to 0-1 range
    f_scores_norm = (f_scores - f_scores.min()) / (f_scores.max() - f_scores.min() + 1e-8)
    mi_scores_norm = (mi_scores - mi_scores.min()) / (mi_scores.max() - mi_scores.min() + 1e-8)
    
    # Combine: 60% F-score + 40% MI (optimal from data creation script)
    combined_scores = 0.6 * f_scores_norm + 0.4 * mi_scores_norm
    top_indices = np.argsort(combined_scores)[-n_features:]
    
    X_train_proc = X_train_scaled[:, top_indices]
    X_val_proc = X_val_scaled[:, top_indices]
    X_test_proc = X_test_scaled[:, top_indices]
    
    print(f"   ‚úÖ Selected {n_features} features, applied robust preprocessing")
    print(f"   üìä Feature range: [{X_train_proc.min():.3f}, {X_train_proc.max():.3f}]")
    
    return X_train_proc, X_val_proc, X_test_proc

def enhanced_training_real_data(model, train_loader, val_loader, y_train, device):
    """Enhanced training strategy optimized for real ABIDE data."""
    print("üéØ Enhanced training (real data optimized)...")
    
    # Class weights for real ABIDE imbalance
    class_counts = np.bincount(y_train.astype(int))
    class_weights = torch.FloatTensor(len(y_train) / (len(class_counts) * class_counts)).to(device)
    print(f"   Class weights: {class_weights}")
    
    # Loss with class weights and label smoothing (proven effective)
    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)
    
    # AdamW with proven hyperparameters
    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4, betas=(0.9, 0.999))
    
    # Learning rate scheduler with warmup (from working notebook)
    def lr_lambda(epoch):
        warmup_epochs = 10
        if epoch < warmup_epochs:
            return (epoch + 1) / warmup_epochs
        return 0.95 ** (epoch - warmup_epochs)
    
    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    
    # Training parameters optimized for real data
    num_epochs = 100  # Sufficient for real data
    best_val_acc = 0
    best_model_state = None
    patience = 20     # More patience for real data
    patience_counter = 0
    
    print(f"   Training for up to {num_epochs} epochs with patience {patience}")
    
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        
        for features, labels in train_loader:
            features, labels = features.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(features)
            loss = criterion(outputs, labels)
            loss.backward()
            
            # Gradient clipping (crucial for stability)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            train_total += labels.size(0)
            train_correct += predicted.eq(labels).sum().item()
        
        # Validation phase
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for features, labels in val_loader:
                features, labels = features.to(device), labels.to(device)
                outputs = model(features)
                loss = criterion(outputs, labels)
                
                val_loss += loss.item()
                _, predicted = outputs.max(1)
                val_total += labels.size(0)
                val_correct += predicted.eq(labels).sum().item()
        
        train_acc = train_correct / train_total
        val_acc = val_correct / val_total
        
        # Early stopping
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_model_state = model.state_dict().copy()
            patience_counter = 0
        else:
            patience_counter += 1
        
        scheduler.step()
        
        if (epoch + 1) % 20 == 0 or epoch < 5:
            print(f"   Epoch {epoch+1:2d}: Train {train_acc:.4f}, Val {val_acc:.4f}")
        
        if patience_counter >= patience:
            print(f"   Early stopping at epoch {epoch+1}")
            break
    
    # Load best model
    if best_model_state:
        model.load_state_dict(best_model_state)
    
    print(f"   ‚úÖ Training completed, best val acc: {best_val_acc:.4f}")
    return best_val_acc

def evaluate_comprehensive(model, test_loader, device):
    """Comprehensive evaluation for real data."""
    model.eval()
    test_correct = 0
    test_total = 0
    all_probs = []
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for features, labels in test_loader:
            features, labels = features.to(device), labels.to(device)
            outputs = model(features)
            probs = torch.softmax(outputs, dim=1)
            _, predicted = outputs.max(1)
            
            test_total += labels.size(0)
            test_correct += predicted.eq(labels).sum().item()
            
            all_probs.extend(probs[:, 1].cpu().numpy())
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    test_acc = test_correct / test_total
    test_auc = roc_auc_score(all_labels, all_probs)
    
    return test_acc, test_auc, all_preds, all_labels

def main():
    """Test improved sMRI system with real ABIDE data."""
    print("üöÄ Testing FIXED sMRI System (Real Data)")
    print("=" * 60)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üì± Device: {device}")
    
    # Load real data
    features, labels, is_real_data = load_real_abide_data()
    data_type = "REAL ABIDE" if is_real_data else "HIGH-FIDELITY SYNTHETIC"
    print(f"üìä Data type: {data_type}")
    print(f"üìä Shape: {features.shape[0]} subjects, {features.shape[1]} features")
    print(f"üéØ Classes: ASD={np.sum(labels==1)}, Control={np.sum(labels==0)}")
    
    # Cross-validation for robust results
    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)
    cv_results = []
    
    print(f"\nüîÑ 3-Fold Cross-Validation...")
    
    for fold, (train_idx, test_idx) in enumerate(skf.split(features, labels)):
        print(f"\n{'='*20} FOLD {fold+1}/3 {'='*20}")
        
        # Split data
        X_train_fold, X_test_fold = features[train_idx], features[test_idx]
        y_train_fold, y_test_fold = labels[train_idx], labels[test_idx]
        
        # Validation split
        X_train, X_val, y_train, y_val = train_test_split(
            X_train_fold, y_train_fold, test_size=0.2, random_state=SEED, stratify=y_train_fold
        )
        
        print(f"Data split: Train={X_train.shape[0]}, Val={X_val.shape[0]}, Test={X_test_fold.shape[0]}")
        
        # Enhanced preprocessing for real data
        X_train_proc, X_val_proc, X_test_proc = enhanced_preprocessing_real_data(
            X_train, X_val, X_test_fold, y_train, n_features=300
        )
        
        # Create datasets with optimal parameters
        train_dataset = SMRIDataset(X_train_proc, y_train, augment=True, noise_factor=0.005)
        val_dataset = SMRIDataset(X_val_proc, y_val, augment=False)
        test_dataset = SMRIDataset(X_test_proc, y_test_fold, augment=False)
        
        # Data loaders with weighted sampling
        class_counts = np.bincount(y_train.astype(int))
        class_weights = 1.0 / class_counts
        sample_weights = class_weights[y_train.astype(int)]
        sampler = WeightedRandomSampler(
            weights=sample_weights, num_samples=len(sample_weights), replacement=True
        )
        
        train_loader = DataLoader(train_dataset, batch_size=16, sampler=sampler)
        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)
        
        # Create IMPROVED model (this is now the fixed version!)
        model = SMRITransformer(
            input_dim=X_train_proc.shape[1],
            d_model=64,
            n_heads=4,
            n_layers=2,
            dropout=0.3,
            layer_dropout=0.1
        ).to(device)
        
        # Verify we're using the improved model
        model_info = model.get_model_info()
        print(f"üß† Model: {model_info['model_name']}")
        print(f"   Parameters: {model_info['total_params']:,}")
        
        # Check if improvements are present
        improvements = model_info.get('improvements', [])
        if improvements:
            print(f"   ‚úÖ Improvements detected:")
            for imp in improvements[:3]:  # Show first 3
                print(f"      ‚Ä¢ {imp}")
        else:
            print(f"   ‚ö†Ô∏è  No improvement metadata found")
        
        # Enhanced training
        best_val_acc = enhanced_training_real_data(model, train_loader, val_loader, y_train, device)
        
        # Test evaluation
        test_acc, test_auc, test_preds, test_labels = evaluate_comprehensive(model, test_loader, device)
        
        cv_results.append({
            'fold': fold + 1,
            'test_acc': test_acc,
            'test_auc': test_auc,
            'val_acc': best_val_acc
        })
        
        print(f"Fold {fold+1} Results: Test Acc: {test_acc:.4f} ({test_acc:.1%}), AUC: {test_auc:.4f}")
    
    # Final results
    mean_test_acc = np.mean([r['test_acc'] for r in cv_results])
    std_test_acc = np.std([r['test_acc'] for r in cv_results])
    mean_test_auc = np.mean([r['test_auc'] for r in cv_results])
    
    print(f"\nüéØ FIXED sMRI SYSTEM - FINAL RESULTS:")
    print("=" * 60)
    print(f"Data Type: {data_type}")
    print(f"Mean Test Accuracy: {mean_test_acc:.4f} ¬± {std_test_acc:.4f} ({mean_test_acc:.1%})")
    print(f"Mean Test AUC: {mean_test_auc:.4f}")
    print(f"Original Baseline: ~52.6%")
    print(f"Improvement: {(mean_test_acc - 0.526)*100:+.1f} percentage points")
    
    # Performance assessment
    if mean_test_acc >= 0.60:
        print(f"üéØ SUCCESS! Achieved 60%+ target!")
        status = "TARGET_ACHIEVED"
    elif mean_test_acc >= 0.58:
        print(f"‚úÖ VERY CLOSE! Near target performance!")
        status = "CLOSE_TO_TARGET"
    elif mean_test_acc >= 0.55:
        print(f"‚úÖ GOOD! Significant improvement!")
        status = "GOOD_IMPROVEMENT"
    else:
        print(f"‚ö†Ô∏è  Progress made, but more work needed...")
        status = "NEEDS_MORE_WORK"
    
    print(f"\nüí° Key Improvements Applied:")
    print(f"   ‚úÖ Working notebook architecture (BatchNorm, GELU, pre-norm)")
    print(f"   ‚úÖ Enhanced preprocessing (RobustScaler, outlier removal)")
    print(f"   ‚úÖ Combined feature selection (F-score + MI)")
    print(f"   ‚úÖ Advanced training (class weights, warmup, early stopping)")
    print(f"   ‚úÖ Real data optimizations (gradient clipping, patience)")
    
    # Comparison with your original results
    print(f"\nüìä Comparison with Your Original Results:")
    print(f"   Your sMRI:        48.97% ¬± 4.7%")
    print(f"   Fixed sMRI:       {mean_test_acc:.2%} ¬± {std_test_acc:.1%}")
    print(f"   Improvement:      {(mean_test_acc - 0.4897)*100:+.1f} percentage points")
    print(f"   Cross-attention:  Expected to improve from 63.6%")
    
    return {
        'mean_accuracy': mean_test_acc,
        'std_accuracy': std_test_acc,
        'mean_auc': mean_test_auc,
        'status': status,
        'data_type': data_type
    }

if __name__ == "__main__":
    results = main()
    print(f"\nüöÄ Fixed sMRI test completed!")
    print(f"   Final accuracy: {results['mean_accuracy']:.1%}")
    print(f"   Status: {results['status']}")
    print(f"   Data: {results['data_type']}")
    print(f"\nüí° Next: Test your full multimodal system - it should now perform much better!") 